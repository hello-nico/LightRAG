#!/usr/bin/env python3
"""
å°†domain_kg_enhanced.jsonè½¬æ¢ä¸ºLightRAGæ ¼å¼çš„è½¬æ¢è„šæœ¬
"""

import json
import uuid
from typing import Dict, List, Any
from collections import defaultdict

def convert_domain_kg_to_lightrag_format(input_file: str, output_file: str):
    """
    å°†domain_kg_enhanced.jsonè½¬æ¢ä¸ºLightRAGæ‰€éœ€çš„æ ¼å¼
    
    Args:
        input_file: è¾“å…¥çš„KGæ–‡ä»¶è·¯å¾„
        output_file: è¾“å‡ºçš„LightRAGæ ¼å¼æ–‡ä»¶è·¯å¾„
    """
    
    # åŠ è½½åŸå§‹æ•°æ®
    with open(input_file, 'r', encoding='utf-8') as f:
        original_data = json.load(f)
    
    domain_name = original_data.get('domain_name', 'RAG')
    entities = original_data.get('entities', [])
    relationships = original_data.get('relationships', [])
    
    print(f"ğŸ”„ å¼€å§‹è½¬æ¢ {len(entities)} ä¸ªå®ä½“å’Œ {len(relationships)} ä¸ªå…³ç³»...")
    
    # åˆ›å»ºLightRAGæ ¼å¼çš„æ•°æ®ç»“æ„
    lightrag_data = {
        "chunks": [],
        "entities": [],
        "relationships": [],
        "metadata": {
            "dataset_name": f"{domain_name} Knowledge Graph",
            "created_date": "2025-01-20",
            "description": f"Converted from domain_kg_enhanced.json with {len(entities)} entities and {len(relationships)} relationships",
            "source_format": "domain_kg_enhanced",
            "total_entities": len(entities),
            "total_relationships": len(relationships),
            "domain": domain_name
        }
    }
    
    # æŒ‰è®ºæ–‡åˆ†ç»„çš„å®ä½“å’Œå…³ç³»
    paper_groups = defaultdict(lambda: {
        'entities': [],
        'relationships': [],
        'papers': []
    })
    
    # é¦–å…ˆæŒ‰è®ºæ–‡åˆ†ç»„
    for entity in entities:
        if entity.get('type') == 'Paper':
            paper_name = entity.get('name')
            paper_groups[paper_name]['papers'].append(entity)
    
    # å°†å®ä½“åˆ†é…åˆ°å¯¹åº”çš„è®ºæ–‡ç»„
    for entity in entities:
        if entity.get('type') != 'Paper':
            # æ ¹æ®æè¿°ä¸­çš„è®ºæ–‡åç§°åˆ†é…åˆ°å¯¹åº”ç»„
            description = entity.get('description', '')
            assigned = False
            
            for paper_name in paper_groups.keys():
                if paper_name in description:
                    paper_groups[paper_name]['entities'].append(entity)
                    assigned = True
                    break
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å¯¹åº”è®ºæ–‡ï¼Œåˆ›å»ºä¸€ä¸ªé€šç”¨ç»„
            if not assigned:
                paper_groups['general']['entities'].append(entity)
    
    # å°†å…³ç³»åˆ†é…åˆ°å¯¹åº”çš„è®ºæ–‡ç»„
    for relationship in relationships:
        src_id = relationship.get('src_id', '')
        tgt_id = relationship.get('tgt_id', '')
        description = relationship.get('description', '')
        assigned = False
        
        for paper_name in paper_groups.keys():
            if paper_name in description or paper_name == src_id or paper_name == tgt_id:
                paper_groups[paper_name]['relationships'].append(relationship)
                assigned = True
                break
        
        if not assigned:
            paper_groups['general']['relationships'].append(relationship)
    
    # ä¸ºæ¯ä¸ªç»„åˆ›å»ºchunkså’Œå¯¹åº”çš„å®ä½“ã€å…³ç³»
    chunk_index = 0
    for group_name, group_data in paper_groups.items():
        if not group_data['entities'] and not group_data['relationships']:
            continue
            
        # åˆ›å»ºchunkå†…å®¹
        if group_name == 'general':
            chunk_content = f"General {domain_name} domain knowledge containing various entities and relationships."
        else:
            # æ‰¾åˆ°å¯¹åº”çš„è®ºæ–‡å®ä½“
            paper_entity = next((p for p in group_data['papers'] if p.get('name') == group_name), None)
            if paper_entity:
                chunk_content = f"Paper: {group_name}\n\n"
                chunk_content += f"Description: {paper_entity.get('description', '')}\n\n"
                
                # æ·»åŠ ç›¸å…³ä½œè€…
                authors = [e for e in group_data['entities'] if e.get('type') == 'Author']
                if authors:
                    chunk_content += "Authors:\n"
                    for author in authors[:5]:  # é™åˆ¶æ˜¾ç¤ºæ•°é‡
                        chunk_content += f"- {author.get('name', '')}\n"
                    chunk_content += "\n"
                
                # æ·»åŠ ç›¸å…³æœºæ„
                institutions = [e for e in group_data['entities'] if e.get('type') == 'Institution']
                if institutions:
                    chunk_content += "Institutions:\n"
                    for inst in institutions[:3]:
                        chunk_content += f"- {inst.get('name', '')}\n"
                    chunk_content += "\n"
                
                # æ·»åŠ ä¸»è¦æ–¹æ³•
                methods = [e for e in group_data['entities'] if e.get('type') == 'Method']
                if methods:
                    chunk_content += "Key Methods:\n"
                    for method in methods[:3]:
                        chunk_content += f"- {method.get('name', '')}\n"
            else:
                chunk_content = f"Knowledge about {group_name} and related entities."
        
        # åˆ›å»ºchunk
        chunk_id = f"chunk_{chunk_index:04d}"
        chunk = {
            "content": chunk_content,
            "source_id": group_name.replace(' ', '_').lower(),
            "file_path": f"domain_kg/{domain_name.lower()}/",
            "chunk_order_index": chunk_index
        }
        lightrag_data["chunks"].append(chunk)
        
        # æ·»åŠ å®ä½“åˆ°LightRAGæ ¼å¼
        for entity in group_data['entities']:
            lightrag_entity = {
                "entity_name": entity.get('name'),
                "entity_type": entity.get('type'),
                "description": entity.get('description', ''),
                "source_id": chunk_id,
                "file_path": f"domain_kg/{domain_name.lower()}/"
            }
            lightrag_data["entities"].append(lightrag_entity)
        
        # æ·»åŠ å…³ç³»åˆ°LightRAGæ ¼å¼
        for relationship in group_data['relationships']:
            # ä»æè¿°ä¸­æå–å…³é”®è¯
            description = relationship.get('description', '')
            keywords = []
            
            # åŸºäºå…³ç³»ç±»å‹æ·»åŠ å…³é”®è¯
            desc_lower = description.lower()
            if 'author' in desc_lower or 'ä½œè€…' in description:
                keywords.extend(['authorship', 'paper', 'research'])
            elif 'institution' in desc_lower or 'æœºæ„' in description:
                keywords.extend(['affiliation', 'organization', 'institution'])
            elif 'component' in desc_lower or 'ç»„æˆéƒ¨åˆ†' in description:
                keywords.extend(['component', 'part', 'architecture'])
            elif 'used' in desc_lower or 'ä½¿ç”¨' in description:
                keywords.extend(['usage', 'implementation', 'tool'])
            elif 'evaluation' in desc_lower or 'è¯„ä¼°' in description:
                keywords.extend(['evaluation', 'metric', 'assessment'])
            elif 'application' in desc_lower or 'åº”ç”¨' in description:
                keywords.extend(['application', 'use-case', 'scenario'])
            elif 'based' in desc_lower or 'åŸºç¡€' in description:
                keywords.extend(['foundation', 'basis', 'underlying'])
            else:
                keywords.extend(['related', 'connection', 'association'])
            
            lightrag_relationship = {
                "src_id": relationship.get('src_id'),
                "tgt_id": relationship.get('tgt_id'),
                "description": description,
                "keywords": ", ".join(keywords),
                "weight": 0.8,  # é»˜è®¤æƒé‡
                "source_id": chunk_id,
                "file_path": f"domain_kg/{domain_name.lower()}/"
            }
            lightrag_data["relationships"].append(lightrag_relationship)
        
        chunk_index += 1
    
    # ä¿å­˜è½¬æ¢åçš„æ•°æ®
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(lightrag_data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… è½¬æ¢å®Œæˆ!")
    print(f"   â€¢ åŸå§‹å®ä½“: {len(entities)}")
    print(f"   â€¢ åŸå§‹å…³ç³»: {len(relationships)}")
    print(f"   â€¢ ç”Ÿæˆchunks: {len(lightrag_data['chunks'])}")
    print(f"   â€¢ è½¬æ¢åå®ä½“: {len(lightrag_data['entities'])}")
    print(f"   â€¢ è½¬æ¢åå…³ç³»: {len(lightrag_data['relationships'])}")
    print(f"   â€¢ è¾“å‡ºæ–‡ä»¶: {output_file}")
    
    return lightrag_data

def validate_converted_data(data: Dict[str, Any]):
    """éªŒè¯è½¬æ¢åçš„æ•°æ®æ ¼å¼"""
    required_fields = {
        'chunks': ['content', 'source_id', 'file_path', 'chunk_order_index'],
        'entities': ['entity_name', 'entity_type', 'description', 'source_id', 'file_path'],
        'relationships': ['src_id', 'tgt_id', 'description', 'keywords', 'weight', 'source_id', 'file_path']
    }
    
    for section, fields in required_fields.items():
        if section not in data:
            raise ValueError(f"Missing section: {section}")
        
        for item in data[section]:
            for field in fields:
                if field not in item:
                    raise ValueError(f"Missing field '{field}' in {section}")
    
    print("âœ… æ•°æ®æ ¼å¼éªŒè¯é€šè¿‡!")

if __name__ == "__main__":
    input_file = "/home/chencheng/py/src/LightRAG/datasets/domain_kg_enhanced.json"
    output_file = "/home/chencheng/py/src/LightRAG/datasets/domain_kg_lightrag_format.json"
    
    try:
        converted_data = convert_domain_kg_to_lightrag_format(input_file, output_file)
        validate_converted_data(converted_data)
        print("\nğŸ‰ æ•°æ®è½¬æ¢æˆåŠŸå®Œæˆ! ç°åœ¨å¯ä»¥åœ¨LightRAGä¸­ä½¿ç”¨äº†ã€‚")
    except Exception as e:
        print(f"âŒ è½¬æ¢å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()