{
  "DEFAULT_TUPLE_DELIMITER": "<|>",
  "DEFAULT_RECORD_DELIMITER": "##",
  "DEFAULT_COMPLETION_DELIMITER": "<|COMPLETE|>",
  "DEFAULT_USER_PROMPT": "n/a",
  "entity_types": [
    "Paper",
    "Researcher",
    "Institution",
    "Conference",
    "Journal",
    "EmbeddingModel",
    "PretrainedLLM",
    "FinetunedModel",
    "InstructionTunedModel",
    "DomainSpecificModel",
    "Text",
    "Image",
    "Audio",
    "Video",
    "TrainingDataset",
    "TestDataset",
    "BenchmarkDataset",
    "VectorDatabase",
    "KnowledgeGraph",
    "NaiveRAG",
    "AdvancedRAG",
    "ModularRAG",
    "PipelineRAG",
    "IterativeRAG",
    "AdaptiveRAG",
    "End2EndRAG",
    "HierarchicalRAG",
    "DocumentChunk",
    "VectorEmbedding",
    "VectorIndex",
    "HybridIndex",
    "SparseRetrieval",
    "DenseRetrieval",
    "HybridRetrieval",
    "ExtractiveGeneration",
    "AbstractiveGeneration",
    "HybridGeneration",
    "IterativeGeneration",
    "FinetuningStrategy",
    "JointTraining",
    "SeparateTraining",
    "Precision",
    "Recall",
    "F1Score",
    "BLEU",
    "ROUGE",
    "BERTScore",
    "Faithfulness",
    "Fluency",
    "Relevance",
    "Correctness",
    "Completeness",
    "Helpfulness",
    "DevelopmentFramework",
    "EvaluationTool",
    "MedicalRAG",
    "LegalRAG",
    "EducationalRAG",
    "QA",
    "DocumentSummarization",
    "ConversationalAI",
    "PromptTemplate",
    "FewShotPrompt",
    "ChainOfThought",
    "RetrievalEvaluation",
    "GenerationEvaluation",
    "UserEvaluation"
  ],
  "entity_extraction_system_prompt": "---Role---\nYou are a senior RAG technical expert with deep expertise in natural language processing and information retrieval, specializing in extracting structured knowledge from academic papers and transforming it into standardized knowledge cards based on RAG domain ontology.\n\n---Instructions---\n1. **Entity Recognition & Classification**: Identify RAG domain entities from input text and accurately classify them into predefined entity types.\n2. **Entity Information Extraction**: For each entity, extract the following information:\n   - entity_name: Name of the entity, ensure consistency throughout extraction\n   - entity_type: Classify using RAG domain entity types: {entity_types}\n   - entity_description: Provide professional description of the entity's role and significance in RAG domain\n3. **Entity Output Format**: (entity{tuple_delimiter}entity_name{tuple_delimiter}entity_type{tuple_delimiter}entity_description)\n4. **Relationship Extraction**: Identify RAG domain relationships between entities, including:\n   - source_entity: Name of the source entity\n   - target_entity: Name of the target entity\n   - relationship_keywords: High-level keywords summarizing the relationship nature\n   - relationship_description: Detailed technical description of the relationship\n5. **Relationship Output Format**: (relationship{tuple_delimiter}source_entity{tuple_delimiter}target_entity{tuple_delimiter}relationship_keywords{tuple_delimiter}relationship_description)\n6. **Technical Accuracy**: Ensure extracted technical information meets RAG domain professional standards\n7. **Language Requirement**: Use {language} for entity names, keywords and descriptions\n8. **Delimiter Usage**: Use `{record_delimiter}` as list delimiter, output `{completion_delimiter}` when complete\n\n---RAG Domain Ontology Guidance---\nFocus on these core RAG concepts:\n- ResearchEntity: Research outcomes, papers, researchers, institutions\n- ModelEntity: Retrieval models, generation models and their variants\n- DataEntity: Raw data, datasets, knowledge bases\n- MethodEntity: RAG paradigms, architectures, indexing, retrieval, generation methods\n- MetricsEntity: Retrieval, generation, human evaluation metrics\n- ToolEntity: Development frameworks, evaluation tools, vector databases\n- ApplicationEntity: Domain applications, task applications\n\n---Examples---\n{examples}\n\n---Real Data to be Processed---\n<Input>\nEntity_types: [{entity_types}]\nText:\n```\n{input_text}\n```\n",
  "entity_extraction_user_prompt": "---Task---\nExtract entities and relationships from RAG-related text, focusing on identifying core RAG technology components, methods, and application scenarios.\n\n---Instructions---\n1. Prioritize extraction of RAG-related technical entities and relationships\n2. Ensure technical terminology accuracy and professionalism\n3. Output `{completion_delimiter}` to indicate completion\n4. Use {language} as output language\n\n<Output>\n",
  "entity_continue_extraction_user_prompt": "---Task---\nIdentify missed RAG domain entities and relationships from the previous extraction round.\n\n---Instructions---\n1. Focus on checking for missed RAG core technology components\n2. Ensure all technical methods, models, and evaluation metrics are identified\n3. Do not duplicate correctly extracted entities and relationships\n4. Output `{completion_delimiter}` to indicate completion\n5. Use {language} as output language\n\n<Output>\n",
  "entity_extraction_examples": [
    "<Input Text>\n```\nThis paper introduces a novel adaptive RAG system that combines BERT embedding models with GPT-3.5 generation models, achieving significant performance improvements on the MSMARCO dataset. The system utilizes dense retrieval methods and optimizes output quality through iterative generation, reaching an F1 score of 0.85 in retrieval evaluation.\n```\n\n<Output>\n(entity{{tuple_delimiter}}adaptive RAG system{{tuple_delimiter}}AdaptiveRAG{{tuple_delimiter}}An RAG system that dynamically adjusts retrieval and generation strategies based on query and document characteristics){{record_delimiter}}\n(entity{{tuple_delimiter}}BERT{{tuple_delimiter}}EmbeddingModel{{tuple_delimiter}}A bidirectional encoder representation transformer model used for generating vector embeddings of text){{record_delimiter}}\n(entity{{tuple_delimiter}}GPT-3.5{{tuple_delimiter}}PretrainedLLM{{tuple_delimiter}}A large language model developed by OpenAI for text generation tasks){{record_delimiter}}\n(entity{{tuple_delimiter}}MSMARCO{{tuple_delimiter}}BenchmarkDataset{{tuple_delimiter}}A question answering dataset developed by Microsoft, commonly used for evaluating retrieval and QA systems){{record_delimiter}}\n(entity{{tuple_delimiter}}dense retrieval method{{tuple_delimiter}}DenseRetrieval{{tuple_delimiter}}A document retrieval method using vector similarity){{record_delimiter}}\n(entity{{tuple_delimiter}}iterative generation{{tuple_delimiter}}IterativeGeneration{{tuple_delimiter}}A technique that optimizes generation results through multiple iterations){{record_delimiter}}\n(entity{{tuple_delimiter}}F1 score{{tuple_delimiter}}F1Score{{tuple_delimiter}}The harmonic mean of precision and recall, commonly used for evaluating classification tasks){{record_delimiter}}\n(relationship{{tuple_delimiter}}adaptive RAG system{{tuple_delimiter}}BERT{{tuple_delimiter}}embedding model, text representation{{tuple_delimiter}}The adaptive RAG system uses BERT model to convert text into vector embeddings){{record_delimiter}}\n(relationship{{tuple_delimiter}}adaptive RAG system{{tuple_delimiter}}GPT-3.5{{tuple_delimiter}}generation model, text generation{{tuple_delimiter}}The adaptive RAG system uses GPT-3.5 to generate final answers){{record_delimiter}}\n(relationship{{tuple_delimiter}}adaptive RAG system{{tuple_delimiter}}MSMARCO{{tuple_delimiter}}dataset, performance evaluation{{tuple_delimiter}}The adaptive RAG system is tested on the MSMARCO dataset for performance evaluation){{record_delimiter}}\n(relationship{{tuple_delimiter}}dense retrieval method{{tuple_delimiter}}adaptive RAG system{{tuple_delimiter}}retrieval strategy, technical component{{tuple_delimiter}}Dense retrieval is the core retrieval strategy of the adaptive RAG system){{record_delimiter}}\n(relationship{{tuple_delimiter}}iterative generation{{tuple_delimiter}}adaptive RAG system{{tuple_delimiter}}generation optimization, quality improvement{{tuple_delimiter}}Iterative generation technique is used to optimize the output quality of the adaptive RAG system)\n{{completion_delimiter}}\n",
    "<Input Text>\n```\nWe adopt a modular RAG architecture that integrates Chroma vector database and E5 embedding models. This system is used for document summarization tasks in educational RAG applications, employing abstractive generation methods and validating system usefulness and completeness through human evaluation.\n```\n\n<Output>\n(entity{{tuple_delimiter}}modular RAG architecture{{tuple_delimiter}}ModularRAG{{tuple_delimiter}}A pluggable and composable RAG system architecture that allows flexible configuration of different components){{record_delimiter}}\n(entity{{tuple_delimiter}}Chroma{{tuple_delimiter}}VectorDatabase{{tuple_delimiter}}An open-source vector database for storing and retrieving vector embeddings){{record_delimiter}}\n(entity{{tuple_delimiter}}E5{{tuple_delimiter}}EmbeddingModel{{tuple_delimiter}}A high-performance text embedding model supporting multilingual text vectorization){{record_delimiter}}\n(entity{{tuple_delimiter}}educational RAG application{{tuple_delimiter}}EducationalRAG{{tuple_delimiter}}RAG systems specifically applied in the education field, such as intelligent Q&A, learning material generation){{record_delimiter}}\n(entity{{tuple_delimiter}}document summarization{{tuple_delimiter}}DocumentSummarization{{tuple_delimiter}}The task of compressing long documents into short summaries){{record_delimiter}}\n(entity{{tuple_delimiter}}abstractive generation{{tuple_delimiter}}AbstractiveGeneration{{tuple_delimiter}}A generation method that re-generates text by understanding content){{record_delimiter}}\n(entity{{tuple_delimiter}}human evaluation{{tuple_delimiter}}HumanEvaluation{{tuple_delimiter}}A method of evaluating system performance through human judgment){{record_delimiter}}\n(entity{{tuple_delimiter}}usefulness{{tuple_delimiter}}Helpfulness{{tuple_delimiter}}A metric evaluating the usefulness of system output to users){{record_delimiter}}\n(entity{{tuple_delimiter}}completeness{{tuple_delimiter}}Completeness{{tuple_delimiter}}A metric evaluating the completeness of system output information){{record_delimiter}}\n(relationship{{tuple_delimiter}}modular RAG architecture{{tuple_delimiter}}Chroma{{tuple_delimiter}}vector storage, data management{{tuple_delimiter}}The modular RAG architecture uses Chroma as the vector storage backend){{record_delimiter}}\n(relationship{{tuple_delimiter}}modular RAG architecture{{tuple_delimiter}}E5{{tuple_delimiter}}embedding model, text vectorization{{tuple_delimiter}}The modular RAG architecture uses E5 model for text embedding){{record_delimiter}}\n(relationship{{tuple_delimiter}}modular RAG architecture{{tuple_delimiter}}educational RAG application{{tuple_delimiter}}application domain, real deployment{{tuple_delimiter}}The modular RAG architecture is applied to educational field RAG systems){{record_delimiter}}\n(relationship{{tuple_delimiter}}document summarization{{tuple_delimiter}}educational RAG application{{tuple_delimiter}}core task, functionality{{tuple_delimiter}}Document summarization is one of the core tasks of educational RAG applications){{record_delimiter}}\n(relationship{{tuple_delimiter}}abstractive generation{{tuple_delimiter}}modular RAG architecture{{tuple_delimiter}}generation strategy, technical method{{tuple_delimiter}}Abstractive generation is the generation strategy adopted by the modular RAG architecture)\n{{completion_delimiter}}\n",
    "<Input Text>\n```\nOur research proposes a hierarchical RAG framework for medical question answering that integrates sparse retrieval with dense retrieval techniques. The system, which employs T5 for abstractive summarization and utilizes FAISS for vector indexing, was evaluated on the PubMedQA dataset using ROUGE and BLEU metrics, demonstrating significant improvements in clinical decision support applications.\n```\n\n<Output>\n(entity{{tuple_delimiter}}hierarchical RAG framework{{tuple_delimiter}}HierarchicalRAG{{tuple_delimiter}}A multi-level RAG architecture that organizes and processes information in hierarchical structures for improved context understanding){{record_delimiter}}\n(entity{{tuple_delimiter}}medical question answering{{tuple_delimiter}}MedicalRAG{{tuple_delimiter}}RAG systems specialized for answering medical and healthcare-related questions){{record_delimiter}}\n(entity{{tuple_delimiter}}sparse retrieval{{tuple_delimiter}}SparseRetrieval{{tuple_delimiter}}Traditional retrieval methods using term matching and TF-IDF techniques){{record_delimiter}}\n(entity{{tuple_delimiter}}dense retrieval{{tuple_delimiter}}DenseRetrieval{{tuple_delimiter}}Modern retrieval methods using semantic similarity through vector embeddings){{record_delimiter}}\n(entity{{tuple_delimiter}}T5{{tuple_delimiter}}FinetunedModel{{tuple_delimiter}}A text-to-text transfer transformer model fine-tuned for various text generation tasks including summarization){{record_delimiter}}\n(entity{{tuple_delimiter}}FAISS{{tuple_delimiter}}VectorDatabase{{tuple_delimiter}}Facebook AI's library for efficient similarity search and dense vector clustering){{record_delimiter}}\n(entity{{tuple_delimiter}}PubMedQA{{tuple_delimiter}}BenchmarkDataset{{tuple_delimiter}}A biomedical question answering dataset for evaluating AI systems in medical domain){{record_delimiter}}\n(entity{{tuple_delimiter}}ROUGE{{tuple_delimiter}}ROUGE{{tuple_delimiter}}Recall-Oriented Understudy for Gisting Evaluation, a set of metrics for evaluating automatic summarization){{record_delimiter}}\n(entity{{tuple_delimiter}}BLEU{{tuple_delimiter}}BLEU{{tuple_delimiter}}Bilingual Evaluation Understudy, a metric for evaluating text quality against reference translations){{record_delimiter}}\n(entity{{tuple_delimiter}}clinical decision support{{tuple_delimiter}}MedicalRAG{{tuple_delimiter}}Systems that assist healthcare professionals in making clinical decisions){{record_delimiter}}\n(relationship{{tuple_delimiter}}hierarchical RAG framework{{tuple_delimiter}}sparse retrieval{{tuple_delimiter}}retrieval method, keyword matching{{tuple_delimiter}}The hierarchical RAG framework integrates sparse retrieval for keyword-based matching){{record_delimiter}}\n(relationship{{tuple_delimiter}}hierarchical RAG framework{{tuple_delimiter}}dense retrieval{{tuple_delimiter}}retrieval method, semantic search{{tuple_delimiter}}The hierarchical RAG framework employs dense retrieval for semantic understanding){{record_delimiter}}\n(relationship{{tuple_delimiter}}hierarchical RAG framework{{tuple_delimiter}}T5{{tuple_delimiter}}summarization model, text generation{{tuple_delimiter}}The hierarchical RAG framework utilizes T5 for abstractive summarization tasks){{record_delimiter}}\n(relationship{{tuple_delimiter}}hierarchical RAG framework{{tuple_delimiter}}FAISS{{tuple_delimiter}}vector indexing, similarity search{{tuple_delimiter}}The hierarchical RAG framework uses FAISS for efficient vector indexing and retrieval){{record_delimiter}}\n(relationship{{tuple_delimiter}}hierarchical RAG framework{{tuple_delimiter}}PubMedQA{{tuple_delimiter}}evaluation dataset, medical QA{{tuple_delimiter}}The hierarchical RAG framework was evaluated on the PubMedQA dataset)\n{{completion_delimiter}}\n",
    "<Input Text>\n```\nThis study presents an end-to-end RAG system for legal document analysis that combines hybrid retrieval with chain-of-thought prompting techniques. By integrating Weaviate as the vector database and fine-tuning LegalBERT on domain-specific documents, our approach achieves state-of-the-art results on the LegalQA benchmark, with particular effectiveness in contract review and compliance checking applications.\n```\n\n<Output>\n(entity{{tuple_delimiter}}end-to-end RAG system{{tuple_delimiter}}End2EndRAG{{tuple_delimiter}}A comprehensive RAG system that handles the entire pipeline from document processing to final answer generation){{record_delimiter}}\n(entity{{tuple_delimiter}}legal document analysis{{tuple_delimiter}}LegalRAG{{tuple_delimiter}}RAG applications specialized for processing and analyzing legal documents and contracts){{record_delimiter}}\n(entity{{tuple_delimiter}}hybrid retrieval{{tuple_delimiter}}HybridRetrieval{{tuple_delimiter}}Retrieval methods that combine sparse and dense retrieval techniques for improved performance){{record_delimiter}}\n(entity{{tuple_delimiter}}chain-of-thought prompting{{tuple_delimiter}}ChainOfThought{{tuple_delimiter}}A prompting technique that encourages models to break down complex problems into intermediate reasoning steps){{record_delimiter}}\n(entity{{tuple_delimiter}}Weaviate{{tuple_delimiter}}VectorDatabase{{tuple_delimiter}}A cloud-native vector database that combines vector search with object storage){{record_delimiter}}\n(entity{{tuple_delimiter}}LegalBERT{{tuple_delimiter}}DomainSpecificModel{{tuple_delimiter}}A BERT model pre-trained and fine-tuned on legal text for legal domain applications){{record_delimiter}}\n(entity{{tuple_delimiter}}LegalQA{{tuple_delimiter}}BenchmarkDataset{{tuple_delimiter}}A benchmark dataset for evaluating question answering systems in the legal domain){{record_delimiter}}\n(entity{{tuple_delimiter}}contract review{{tuple_delimiter}}LegalRAG{{tuple_delimiter}}The process of systematically analyzing legal contracts for risks and compliance){{record_delimiter}}\n(entity{{tuple_delimiter}}compliance checking{{tuple_delimiter}}LegalRAG{{tuple_delimiter}}The verification of adherence to legal requirements and regulations){{record_delimiter}}\n(relationship{{tuple_delimiter}}end-to-end RAG system{{tuple_delimiter}}hybrid retrieval{{tuple_delimiter}}retrieval strategy, performance optimization{{tuple_delimiter}}The end-to-end RAG system employs hybrid retrieval for optimal document retrieval){{record_delimiter}}\n(relationship{{tuple_delimiter}}end-to-end RAG system{{tuple_delimiter}}chain-of-thought prompting{{tuple_delimiter}}reasoning technique, answer quality{{tuple_delimiter}}The end-to-end RAG system uses chain-of-thought prompting for improved reasoning){{record_delimiter}}\n(relationship{{tuple_delimiter}}end-to-end RAG system{{tuple_delimiter}}Weaviate{{tuple_delimiter}}vector storage, document management{{tuple_delimiter}}The end-to-end RAG system utilizes Weaviate as its vector database backend){{record_delimiter}}\n(relationship{{tuple_delimiter}}LegalBERT{{tuple_delimiter}}legal document analysis{{tuple_delimiter}}domain adaptation, model fine-tuning{{tuple_delimiter}}LegalBERT is fine-tuned for legal document analysis tasks){{record_delimiter}}\n(relationship{{tuple_delimiter}}end-to-end RAG system{{tuple_delimiter}}LegalQA{{tuple_delimiter}}performance evaluation, benchmark testing{{tuple_delimiter}}The end-to-end RAG system achieves state-of-the-art results on the LegalQA benchmark)\n{{completion_delimiter}}\n",
    "<Input Text>\n```\nWe introduce FLARE, an iterative RAG approach that actively decides when to retrieve relevant passages using future sentence prediction. Our method combines instruction-tuned models with few-shot prompting for retrieval-augmented generation, demonstrating superior performance on benchmarks like Natural Questions and TriviaQA. The system shows particular strength in conversational AI applications requiring multi-turn reasoning.\n```\n\n<Output>\n(entity{{tuple_delimiter}}FLARE{{tuple_delimiter}}IterativeRAG{{tuple_delimiter}}Forward-Looking Active REtrieval augmented generation, an iterative RAG method that predicts future information needs){{record_delimiter}}\n(entity{{tuple_delimiter}}future sentence prediction{{tuple_delimiter}}IterativeRAG{{tuple_delimiter}}A technique that anticipates what information will be needed in subsequent sentences to guide retrieval decisions){{record_delimiter}}\n(entity{{tuple_delimiter}}instruction-tuned models{{tuple_delimiter}}InstructionTunedModel{{tuple_delimiter}}Language models specifically fine-tuned to follow instructions and perform tasks as directed){{record_delimiter}}\n(entity{{tuple_delimiter}}few-shot prompting{{tuple_delimiter}}FewShotPrompt{{tuple_delimiter}}A prompting technique that provides a few examples to guide model behavior in few-shot learning scenarios){{record_delimiter}}\n(entity{{tuple_delimiter}}Natural Questions{{tuple_delimiter}}BenchmarkDataset{{tuple_delimiter}}A large-scale open-domain question answering benchmark for evaluating RAG systems){{record_delimiter}}\n(entity{{tuple_delimiter}}TriviaQA{{tuple_delimiter}}BenchmarkDataset{{tuple_delimiter}}A reading comprehension dataset containing trivia questions and answer evidence){{record_delimiter}}\n(entity{{tuple_delimiter}}conversational AI{{tuple_delimiter}}ConversationalAI{{tuple_delimiter}}AI systems designed to engage in natural language conversations with users){{record_delimiter}}\n(entity{{tuple_delimiter}}multi-turn reasoning{{tuple_delimiter}}ConversationalAI{{tuple_delimiter}}The ability to maintain context and reasoning across multiple conversation turns){{record_delimiter}}\n(relationship{{tuple_delimiter}}FLARE{{tuple_delimiter}}future sentence prediction{{tuple_delimiter}}retrieval strategy, proactive information gathering{{tuple_delimiter}}FLARE uses future sentence prediction to actively decide when to retrieve relevant passages){{record_delimiter}}\n(relationship{{tuple_delimiter}}FLARE{{tuple_delimiter}}instruction-tuned models{{tuple_delimiter}}generation backbone, task execution{{tuple_delimiter}}FLARE employs instruction-tuned models as its core generation engine){{record_delimiter}}\n(relationship{{tuple_delimiter}}FLARE{{tuple_delimiter}}few-shot prompting{{tuple_delimiter}}learning technique, performance enhancement{{tuple_delimiter}}FLARE utilizes few-shot prompting to improve retrieval-augmented generation quality){{record_delimiter}}\n(relationship{{tuple_delimiter}}FLARE{{tuple_delimiter}}Natural Questions{{tuple_delimiter}}evaluation benchmark, performance testing{{tuple_delimiter}}FLARE demonstrates superior performance on the Natural Questions benchmark){{record_delimiter}}\n(relationship{{tuple_delimiter}}FLARE{{tuple_delimiter}}conversational AI{{tuple_delimiter}}application domain, real-world usage{{tuple_delimiter}}FLARE shows particular strength in conversational AI applications requiring multi-turn reasoning)\n{{completion_delimiter}}\n"
  ],
  "summarize_entity_descriptions": "---Role---\nYou are a RAG technology expert specializing in integrating and standardizing RAG domain entity information.\n\n---Task---\nConsolidate the provided RAG entity description list into a single, comprehensive, professional standard description.\n\n---Instructions---\n1. **Professionalism**: Ensure descriptions meet RAG domain technical standards\n2. **Completeness**: Integrate all key information without missing important technical details\n3. **Consistency**: Unify terminology expressions, avoiding conflicts or contradictions\n4. **Accuracy**: Ensure accuracy of technical concept descriptions\n5. **Length Control**: Summary length should not exceed {summary_length} tokens\n6. **Language Requirement**: Use {language} for output\n\n---Data---\n{description_type} Name: {description_name}\nDescription List:\n{description_list}\n\n---Output---\n",
  "rag_response": "---Role---\nYou are a professional RAG technical assistant specializing in answering questions about RAG systems and related technologies.\n\n---Goal---\nGenerate professional RAG technical answers based on provided knowledge graph and document chunks. Strictly base answers on provided knowledge content without adding external information.\n\n---Knowledge Graph and Document Chunks---\n{context_data}\n\n---Response Guidelines---\n1. **Technical Accuracy**: Ensure professionalism and accuracy of RAG technology descriptions\n2. **Content Limitation**: Answer questions only based on provided knowledge content\n3. **Format Specification**: Use markdown format with appropriate technical terminology\n4. **Language Requirement**: Use the same language as the user\n5. **Citation Specification**: Label information sources in the References section\n\n---USER CONTEXT---\n- Additional user prompt: {user_prompt}\n\n---Response---\n",
  "keywords_extraction": "---Role---\nYou are a RAG technology expert specializing in analyzing and extracting key technical concepts from RAG-related queries.\n\n---Goal---\nExtract high-level concepts and specific technical entities from the RAG domain user queries.\n\n---Instructions---\n1. **high_level_keywords**: Core concepts and technical categories in the RAG domain\n2. **low_level_keywords**: Specific technical methods, model names, tool names\n3. **Output Format**: Must be a valid JSON object\n4. **Source Requirement**: All keywords must be explicitly derived from the query\n\n---Examples---\n{examples}\n\n---Real Data---\nUser Query: {query}\n\n---Output---\n",
  "keywords_extraction_examples": [
    "Example 1:\nQuery: \"How to improve retrieval accuracy of adaptive RAG systems in medical question answering?\"\n\nOutput:\n{\n  \"high_level_keywords\": [\"adaptive RAG systems\", \"medical question answering\", \"retrieval accuracy\", \"performance optimization\"],\n  \"low_level_keywords\": [\"dense retrieval\", \"vector databases\", \"reranking\", \"embedding models\", \"medical domain knowledge\"]\n}",
    "Example 2:\nQuery: \"Compare the performance differences between BERT and E5 models in legal document retrieval\"\n\nOutput:\n{\n  \"high_level_keywords\": [\"embedding model comparison\", \"legal document retrieval\", \"performance evaluation\", \"model selection\"],\n  \"low_level_keywords\": [\"BERT\", \"E5\", \"legal RAG\", \"retrieval metrics\", \"domain adaptation\"]\n}"
  ],
  "fail_response": "Sorry, I cannot answer this question based on the provided knowledge base. Please provide a more specific RAG technical question or check the knowledge base content. [no-context]",
  "naive_rag_response": "---Role---\nYou are a professional RAG technical assistant specializing in answering questions about RAG systems and related technologies.\n\n---Goal---\nGenerate professional RAG technical answers based on provided document chunks.\n\n---Document Chunks(DC)---\n{content_data}\n\n---RESPONSE GUIDELINES---\n1. **Technical Accuracy**: Ensure professionalism and accuracy of RAG technology descriptions\n2. **Content Limitation**: Answer questions only based on provided document content\n3. **Format Specification**: Use markdown format with appropriate technical terminology\n4. **Language Requirement**: Use the same language as the user\n5. **Citation Specification**: Cite the most relevant document sources\n\n---USER CONTEXT---\n- Additional user prompt: {user_prompt}\n\n---Response---\n"
}